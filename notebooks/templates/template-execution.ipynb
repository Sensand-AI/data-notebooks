{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution template jupyter notebooks - draft\n",
    "\n",
    "This notebook is a generic template for the data notebooks repository. Use this template as a starting point so that as the notebook library grows, there is consistency in formatting as well as the inclusion of required core packages, parameters and tags.\n",
    "PLease refer to the README.md for more information and a more detailed breakdown of ths templates.\n",
    "\n",
    "## Execution templates\n",
    "\n",
    "Exploration notebook templates have some additional requirements over the exploration notebooks.\n",
    "\n",
    "exploraion notebooks must include papermill comments and tags.\n",
    "\n",
    "### Parameter definition and tags\n",
    "\n",
    "For execurtion notebooks to work, both `papermill comments` and `cell tags` must be included. Each cell in this template has these included. `Papermill comments` are different to regular comments, though the syntax is the same. Take care to not remove the papermill comments when cleaning up a notebook for production.\n",
    "\n",
    "### Parameter cell\n",
    "One cell must be designated `parameters`. This cell must have BOTH the `papermill comment` designating it as the aprameters cell, and the `tag`\n",
    "\n",
    "Example:\n",
    "\n",
    "#### papermill comment:\n",
    "`papermill_description=parameters`\n",
    "\n",
    "#### Jupyter tags in vscode:\n",
    "- use the 'more actions' button in the top right of a cell to access tags\n",
    "- select `add cell tag`\n",
    "- make cell tag `parameters`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Library import\n",
    "\n",
    "### Install packages\n",
    "Install any required packages if they aren't included in the data-notebooks environment. \n",
    "To avoid rebuilding the dev container continuously, you should install packages within the data notebook itself. If  a package is being used consistentlya cross multiple notebooks, then it can be considered for inclusion in the dev container.\n",
    "\n",
    "### Import packages\n",
    "Import required python packaged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add papermill_description to every cell\n",
    "add tags to every cell. \n",
    "Parameters cell MUST have both params tag and papermill description\n",
    "notebook key - for executable notebooks\n",
    "\n",
    "The papaermill things are comments and are called comments, and these are separate to tags.\n",
    "\n",
    "exploratory vs executbale notebook template. Make example of both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-py"
    ]
   },
   "outputs": [],
   "source": [
    "#papermill_description=imports\n",
    "\n",
    "# Core packages\n",
    "import json\n",
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Geospatial\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import pystac_client\n",
    "import rioxarray\n",
    "from gis_utils.stac import initialize_stac_client, query_stac_api\n",
    "from geodata_fetch import settingshandler, harvest\n",
    "\n",
    "# Visualisations in notebook\n",
    "from IPython.display import display, JSON\n",
    "import holoviews as hv\n",
    "import geoviews as gv\n",
    "import panel as pn\n",
    "\n",
    "# Exporting data\n",
    "import boto3\n",
    "from aws_utils import S3Utils\n",
    "\n",
    "# this is a GDAL flag, it does not impact AWS access.  Used for accessing public buckets, which we do for some AWS earth data repositories\n",
    "os.environ['AWS_NO_SIGN_REQUEST'] = 'YES'\n",
    "\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-py"
    ]
   },
   "outputs": [],
   "source": [
    "#papermill_description=get_bbox_from_geodf\n",
    "\n",
    "def get_bbox_from_geodf(geojson_data):\n",
    "    \"\"\"\n",
    "    Extract the bounding box from a GeoJSON-like dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    - geojson_data (dict): The GeoJSON data as a Python dictionary.\n",
    "    \n",
    "    Returns:\n",
    "    - A list representing the bounding box [min_lon, min_lat, max_lon, max_lat].\n",
    "    \"\"\"\n",
    "    gdf = gpd.GeoDataFrame.from_features(geojson_data[\"features\"])\n",
    "    bbox = list(gdf.total_bounds)\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "#papermill_description=compute_elevation_statistics\n",
    "\n",
    "def compute_elevation_statistics(dem_data):\n",
    "    \"\"\"\n",
    "    Compute basic elevation statistics from a digital elevation model (DEM) dataset.\n",
    "\n",
    "    This function calculates the minimum, maximum, mean, and standard deviation of elevation\n",
    "    values within the provided DEM data array. It handles the DEM data as a NumPy array,\n",
    "    which is a common format for raster data in Python.\n",
    "\n",
    "    Parameters:\n",
    "    - dem_data (numpy.ndarray): A 2D NumPy array containing elevation data from a DEM raster.\n",
    "      The array should contain numeric values representing elevation at each cell. No-data\n",
    "      values should be represented by NaNs in the array to be properly ignored in calculations.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing the computed elevation statistics, with keys 'min_elevation',\n",
    "      'max_elevation', 'mean_elevation', and 'std_dev_elevation'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the minimum elevation, ignoring any NaN values which represent no-data cells\n",
    "    min_elevation = float(np.nanmin(dem_data))\n",
    "\n",
    "    # Compute the maximum elevation, ignoring any NaN values\n",
    "    max_elevation = float(np.nanmax(dem_data))\n",
    "\n",
    "    # Compute the mean elevation, ignoring any NaN values\n",
    "    mean_elevation = float(np.nanmean(dem_data))\n",
    "\n",
    "    # Compute the standard deviation of elevation, ignoring any NaN values\n",
    "    std_dev_elevation = float(np.nanstd(dem_data))\n",
    "\n",
    "    # Construct and return a dictionary containing the computed statistics\n",
    "    stats = {\n",
    "        'min_elevation': min_elevation,\n",
    "        'max_elevation': max_elevation,\n",
    "        'mean_elevation': mean_elevation,\n",
    "        'std_dev_elevation': std_dev_elevation\n",
    "    }\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "#papermill_description=map_visualisation\n",
    "\n",
    "def list_tif_files(path):\n",
    "    return [f for f in os.listdir(path) if f.endswith('cm.tif')]\n",
    "\n",
    "# Function to load and display the selected .tif file\n",
    "def load_and_display_tif(filename):\n",
    "    filepath = os.path.join(path_settings, filename)\n",
    "    img = gv.util.from_xarray(rioxarray.open_rasterio(filepath).rio.reproject('EPSG:3857'))\n",
    "    \n",
    "    # Define map tiles and create the map image\n",
    "    map_tiles = gv.tile_sources.EsriImagery().opts(width=1000, height=600)\n",
    "    map_img = gv.Image(img, kdims=['x', 'y']).opts(cmap='viridis', title=filename)\n",
    "    map_combo = map_tiles * map_img\n",
    "    \n",
    "    return map_combo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "#papermill_description=processing_file_io\n",
    "\n",
    "# paths for input and output directories.\n",
    "input_dir = '/workspace/notebooks/sandbox/data/input-data'\n",
    "output_dir = '/workspace/notebooks/sandbox/data/output-data'\n",
    "\n",
    "# Path for the input geojson file. This file will then be imported as a geodataframe\n",
    "input_geojson_filename = 'dissolved-boundaries.geojson'\n",
    "input_geom = os.path.join(input_dir, input_geojson_filename)\n",
    "\n",
    "# filename for the getdata harvester settings that will be generated from parameters.\n",
    "geodata_params_fname = 'settings_showcase.json'\n",
    "geodata_params = os.path.join(output_dir,geodata_params_fname)\n",
    "\n",
    "\n",
    "property_name = \"test_farm\"\n",
    "notebook_key = \"localjupyter\"\n",
    "\n",
    "\n",
    "# Import the chosen geometry file as a geodataframe\n",
    "geom = gpd.read_file(input_geom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-py"
    ]
   },
   "outputs": [],
   "source": [
    "#papermill_description=process_input_geometry\n",
    "\n",
    "# Setting parameters to create the settings.json file\n",
    "\n",
    "# Column names for latitude and longitude in input file:\n",
    "colname_lat = geom.centroid.y[0]\n",
    "colname_lng = geom.centroid.x[0]\n",
    "\n",
    "# Bounding box: Left (min Long), Bottom (min Lat), Right (max Long), Top (max Lat)\n",
    "target_bbox = list(geom.total_bounds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-py"
    ]
   },
   "outputs": [],
   "source": [
    "#papermill_description=processing_geodata_input_parameters\n",
    "\n",
    "\n",
    "# Resolution of data download in arcseconds (1 arcsec ~ 30m)\n",
    "target_res = 3\n",
    "\n",
    "date_start = \"2022-10-01\"\n",
    "date_end = \"2022-11-30\"\n",
    "# Number of time intervals to split the image collection into\n",
    "time_intervals = 0\n",
    "\n",
    "# This example is only selecting one target source. See other example notebooks for more complicated examples of fetching multiple data sources\n",
    "target_sources = {\"DEM\":\"DEM\"}\n",
    "\n",
    "json_data = {\n",
    "    \"infile\": property_name,\n",
    "    \"outpath\": output_dir,\n",
    "    \"target_centroid_lat\": colname_lat,\n",
    "    \"target_centroid_lng\": colname_lng,\n",
    "    \"target_bbox\": target_bbox,\n",
    "    \"target_res\": str(target_res),\n",
    "    \"date_start\": date_start if date_start is not None else \"2022-10-01\", #a date of some kind must be provided or the harvester complains\n",
    "    \"date_end\": date_end if date_end is not None else \"2022-11-30\",\n",
    "    \"time_intervals\": time_intervals,\n",
    "    \"target_sources\": target_sources\n",
    "}\n",
    "\n",
    "#write out the parameters as a json file. This can be replaced with an API call at a later date.\n",
    "\n",
    "with open(geodata_params, \"w\", encoding='utf-8') as file:\n",
    "    json.dump(json_data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#papermill_description=parameters\n",
    "\n",
    "# store settings as namespace (easier to interact with)\n",
    "settings = settingshandler.main(geodata_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-py"
    ]
   },
   "outputs": [],
   "source": [
    "#papermill_description=geodata_collection\n",
    "\n",
    "df = harvest.run(geodata_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation and export\n",
    "\n",
    "While you can include visualisation cells in the production notebooks, they do slow down the notebook execution and so should be commented out or controlled using boolean flags. This way they can be used during local testing, and turned off in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=processing_s3\n",
    "\n",
    "# Load AWS credentials from environment variables\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "aws_default_region = 'us-east-1'\n",
    "bucket_name = 'jenna-remote-sensing-sandbox'\n",
    "\n",
    "s3_client = S3Utils(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=aws_default_region,\n",
    "    s3_bucket='jenna-remote-sensing-sandbox',\n",
    "    prefix=notebook_key\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=elevation_process_variables\n",
    "\n",
    "# Construct the filenames using propertyName\n",
    "elevation_json_filename = f\"dem_{property_name}_elevation_stats.json\"\n",
    "\n",
    "#output tif name hardcoded for now but will be dynamically read later\n",
    "output_tiff_filename = 'DEM_SRTM_1_Second_Hydro_Enforced.tif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the collected DEM and comput some statistics on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=read_in_tif_to_compute_stats\n",
    "\n",
    "\n",
    "# Use visualisation to check the input file is correct\n",
    "dem_tiff_dir = os.path.join(output_dir, output_tiff_filename)\n",
    "data = rioxarray.open_rasterio(dem_tiff_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=calculate_elevation_stats\n",
    "\n",
    "\n",
    "elevation_stats = compute_elevation_statistics(data)\n",
    "\n",
    "# Serialize 'elevation_stats' to a JSON string\n",
    "elevation_stats_json = json.dumps(elevation_stats)\n",
    "# Convert the JSON string to bytes\n",
    "elevation_stats_bytes = elevation_stats_json.encode()\n",
    "\n",
    "# print elevation stats as a check:\n",
    "elevation_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to AWS S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(file_path=dem_tiff_dir, file_name=output_tiff_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.list_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.generate_presigned_urls()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
